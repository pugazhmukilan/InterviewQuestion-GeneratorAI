Questions,Answers
What is the core idea behind machine learning as described in the text?,"The core idea behind machine learning as described in the text is to learn representations from data. It involves devising a parametric model *f* with trainable parameters *w* that, with proper values *w*<sup>*</sup>, becomes a good predictor of a quantity *y* given a signal *x*. This ""goodness"" is formalized with a loss function ℒ(w), which is minimized to find the optimal parameters *w*<sup>*</sup>."
"How does basis function regression work, and what is its purpose?","Basis function regression involves expressing a function *f* as a linear combination of predefined basis functions *f1, ..., fK*, with weights *w = (w1, ..., wK)*. The goal is to find the optimal weights *w* that minimize a loss function, typically the mean squared error, between the predicted values *f(x; w)* and the actual values *y* in a training set. In essence, it approximates the relationship between input and output by combining these basis functions in a linear manner."
Explain the concepts of underfitting and overfitting in the context of machine learning models.,"Underfitting: When the capacity of the model is insufficient, the model cannot fit the data, resulting in a high error during training.

Overfitting: When the amount of data is insufficient, the model will often learn characteristics specific to the training examples, resulting in excellent performance during training, at the cost of a worse fit to the global structure of the data, and poor performance on new inputs."
"What are GPUs and TPUs, and why are they relevant to efficient computation in deep learning?","GPUs (Graphical Processing Units) were originally designed for real-time image synthesis and have highly parallel architectures well-suited for deep models. As their usage for AI has increased, GPUs have been equipped with dedicated tensor cores. TPUs (Tensor Processing Units) are deep-learning specialized chips developed by Google.

GPUs and TPUs are relevant to efficient computation in deep learning because deep learning involves executing heavy computations with large amounts of data, and these processors allow such computations to be run on affordable hardware. A GPU possesses several thousand parallel units and its own fast memory. The structure of the GPU itself involves multiple levels of cache memory, which are smaller but faster, and computation should be organized to avoid copies between these different caches."
Describe the role and importance of tensors in deep learning.,"Tensors are series of scalars arranged along several discrete axes, generalizing vectors and matrices. They are used to represent signals to be processed, trainable parameters of models, and intermediate quantities computed by the models (activations). Tensors are instrumental in achieving computational efficiency in deep learning. Deep learning frameworks like PyTorch and JAX manipulate data as tensors, allowing for optimized designs and efficient computation. The implementation of tensors separates the shape representation from the storage layout of coefficients in memory, which allows many reshaping, transposing, and extraction operations to be done without coefficient copying, hence extremely rapidly. Virtually any computation can be decomposed into elementary tensor operations, which avoids non-parallel loops at the language level and poor memory management."
"What is gradient descent, and how does it contribute to training deep learning models?","Gradient descent is an iterative optimization algorithm used to find the minimum of a function. In the context of training deep learning models, it's used to minimize the loss function by iteratively adjusting the model's parameters in the direction of the negative gradient. This involves computing the gradient of the loss with respect to the parameters and subtracting a fraction of it (determined by the learning rate) from the parameters. This process gradually improves the model's performance by reducing the error between its predictions and the actual values."
Explain the backpropagation algorithm and its significance in training neural networks.,"The backpropagation algorithm is a method for efficiently computing the gradients of a loss function with respect to the parameters of a neural network. It consists of a forward pass and a backward pass.

During the forward pass, the input is fed through the network, layer by layer, and the activations at each layer are computed and stored.

During the backward pass, the gradient of the loss function with respect to the output of the network is computed first. Then, using the chain rule, this gradient is propagated backward through the network, layer by layer, to compute the gradients of the loss function with respect to the activations and the parameters of each layer. These gradients are then used to update the parameters of the network using gradient descent or one of its variants.

Backpropagation is significant because it provides an efficient way to train complex neural networks with many layers. Without backpropagation, training such networks would be computationally infeasible."
What is the purpose of activation functions in neural networks? Give some examples.,"Activation functions introduce non-linear operations into the network, which is essential because a network combining only linear components would itself be a linear operator. Examples of activation functions include ReLU, Tanh, Leaky ReLU, and GELU."
Describe the concept of convolutional networks and their applications.,"Convolutional networks, or convnets, are a standard architecture for processing images. They combine multiple convolutional layers, either to reduce the signal size before it can be processed by fully connected layers, or to output a 2D signal also of large size. They are applied in image denoising, image classification, and object detection."
Explain the concept of attention models and their applications.,"Attention layers specifically address the problem of combining local information at locations far apart in a tensor by computing an attention score for each component of the resulting tensor to each component of the input tensor, without locality constraints, and averaging the features across the full tensor accordingly.

The attention operator computes a tensor Y = att(Q,K,V) of dimension NQ×DV, given a tensor Q of queries of size NQ×DQK, a tensor K of keys of size NKV×DQK, and a tensor V of values of size NKV×DV. It computes for every query index q and every key index k an attention score Aq,k as the softargmax of the dot products between the query Qq and the keys. Then a retrieved value is computed for each query by averaging the values according to the attention scores.

The attention operator can be extended by masking the attention matrix before the softargmax normalization by a Boolean matrix M. This allows, for instance, to make the operator causal by taking M full of 1s below the diagonal and zero above, preventing Yq from depending on keys and values of indices k greater than q. The attention matrix is processed by a dropout layer before being multiplied by V, providing the usual benefits during training.

Applications:
Attention layers have become a standard element in many recent models, in particular, the key building block of Transformers, the dominant architecture for Large Language Models."
